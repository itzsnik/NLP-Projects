{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dfa0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "(5572, 2)\n",
      "  Value                                               Text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: Value, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Loading the data into the environment using pandas\n",
    "sms_data = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
    "# Review the loaded data\n",
    "print(sms_data.head())\n",
    "cols = sms_data.columns[:2]\n",
    "data = sms_data[cols]\n",
    "print(data.shape)\n",
    "data = data.rename(columns={\"v1\":\"Value\",\"v2\":\"Text\"})\n",
    "print(data.head())\n",
    "print(data.Value.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da933b",
   "metadata": {},
   "source": [
    "#### Step 2: Feature Engineering\n",
    "Although this step is titled feature engineering, this includes necessary data cleaning and normalizing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a322d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsni\\AppData\\Local\\Temp/ipykernel_15104/4211892017.py:8: FutureWarning: Possible set intersection at position 5\n",
      "  data[\"Punctuations\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[^\\w+&&^\\s]\",x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Value                                               Text  Punctuations  \\\n",
      "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!             2   \n",
      "15  spam  XXXMobileMovieClub: To use your credit, click ...            11   \n",
      "16   ham                         Oh k...i'm watching here:)             6   \n",
      "17   ham  Eh u remember how 2 spell his name... Yes i di...             5   \n",
      "18   ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...             1   \n",
      "19  spam  England v Macedonia - dont miss the goals/team...             7   \n",
      "20   ham          Is that seriously how you spell his name?             1   \n",
      "21   ham  IÛ÷m going to try for 2 months ha ha only joking             2   \n",
      "22   ham  So Ì_ pay first lar... Then when is da stock c...             6   \n",
      "23   ham  Aft i finish my lunch then i go str down lor. ...             3   \n",
      "24   ham  Ffffffffff. Alright no way I can meet up with ...             2   \n",
      "\n",
      "    Phonenumbers  Links  Uppercase  unusualwords  \n",
      "14             0      0          8             0  \n",
      "15             0      1          1             3  \n",
      "16             0      0          0             0  \n",
      "17             0      0          0             0  \n",
      "18             0      0          0             2  \n",
      "19             0      0          2             6  \n",
      "20             0      0          0             0  \n",
      "21             0      0          0             2  \n",
      "22             0      0          1             1  \n",
      "23             0      0          1             4  \n",
      "24             0      0          1             1  \n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "punctuation = list(punctuation)\n",
    "# Creating a new feature called Punctuations. This feature counts the number of punctuation characters in the sms message \n",
    "data[\"Punctuations\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[^\\w+&&^\\s]\",x)))\n",
    "# Creating a new feature called Phonenumbers indicates if the sms text contains a phonenumber or not\n",
    "data[\"Phonenumbers\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[0-9]{10}\",x)))\n",
    "# Creating a new feature called Links feature indicates if the sms text contains a URL or not \n",
    "is_link = lambda x: 1 if re.search(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\",x)!=None else 0\n",
    "data[\"Links\"] = data[\"Text\"].apply(is_link)\n",
    "# Creating a new feature called Uppercase.\n",
    "# This feature indicates how many words in the the sms text are in upper case\n",
    "count_upper = lambda x : list(map(str.isupper,x.split())).count(True)\n",
    "upper_case = lambda y,n : n+1 if y.isupper() else n\n",
    "data[\"Uppercase\"] = data[\"Text\"].apply(count_upper)\n",
    "# Identifying and counting how many unusual words are there in the sms text\n",
    "def find_unusual_words(text):\n",
    "    text_vocab_set = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_set = text_vocab_set - english_vocab_set\n",
    "    return len(sorted(unusual_set))\n",
    "data[\"unusualwords\"] = data[\"Text\"].apply(lambda x: find_unusual_words(word_tokenize(x)))\n",
    "#View a few records of the data after creating these features\n",
    "print(data[14:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a4f8d",
   "metadata": {},
   "source": [
    "In the above code snippet, we have created new features by understanding the content of our data. This is a critical exercise to undergo when you are building NLP applications.\n",
    "\n",
    "The below code snippet converts the text into a TF-IDF matrix. Recall that the TF-IDF matrix is a numeric representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa57e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf= TfidfVectorizer(stop_words=\"english\",strip_accents='ascii',max_features=300)\n",
    "tf_idf_matrix = tf_idf.fit_transform(data[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f29a6",
   "metadata": {},
   "source": [
    "TF-IDF vectorization also does some of the required cleaning and normalization steps such as removing punctuation, removing stop words, removing accents, etc. In the above snippet, we have set the value of max_features to 300, indicating that the TF-IDF matrix contain only the 300 most common words in the text. Doing this reduces the dimensionality of the TF-IDF vector. \n",
    "\n",
    "Finally, the below code snippet, combines the TF-IDF matrix and the other features we created earlier into a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99acdf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extra_features = pd.concat([data,pd.DataFrame(tf_idf_matrix.toarray(),columns=tf_idf.get_feature_names())],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7da4f3",
   "metadata": {},
   "source": [
    "#### Step 3: Machine Learning\n",
    "We converted our textual data into a type that can be used by machine learning algorithms, let us go ahead and apply machine learning. Here we aim to train the machine to recognize which messages are 'spam' and which are 'ham'\n",
    "\n",
    "Recall that before we build a machine learning model, we split the data into train and test sets. The model is learnt on the train set and is validated against the test set.\n",
    "\n",
    "Splits the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c5dc233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Value_num' in  X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e4714ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=data_extra_features\n",
    "features = X.columns.drop([\"Value\",\"Text\"])\n",
    "target = [\"Value\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X[features],X[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d75810",
   "metadata": {},
   "source": [
    "Since the task here is to distinguish one class of messages from another (spam from ham), we need to use classification algorithms.\n",
    "\n",
    "We are using a DecisionTreeClassifier to learn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f112a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837281646326872\n",
      "0.9741564967695621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "dt = DecisionTreeClassifier(min_samples_split=40)\n",
    "dt.fit(X_train,y_train)\n",
    "pred = dt.predict(X_test)\n",
    "print(accuracy_score(y_train, dt.predict(X_train)))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c1df8",
   "metadata": {},
   "source": [
    "We are building 2 more classifier models - Naive Bayes Classifier and Maximum Entropy Classifier (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1743b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968413496051687\n",
      "0.9842067480258435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Building a Naive Bayes Model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "pred_mnb = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, pred_mnb))\n",
    "# Building a Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred_lr = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, pred_lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
